{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install -U spacy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pprint\nimport string\nimport re\nimport pandas as pd\nimport plotly.express as px\n\nimport nltk\nfrom nltk.text import Text\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.collocations import BigramCollocationFinder,BigramAssocMeasures\nimport spacy\nfrom spacy import displacy\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nnltk.download('all')\npp = pprint.PrettyPrinter(indent=4, compact=True)\n#nlp = spacy.load(\"es_core_news_lg\")\n\n# Stop words del español\nenglish_stops = set(stopwords.words('english'))\nenglish_stops.update(string.punctuation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descr_df = pd.read_csv('../input/ted-talks/ted_main.csv')\ntrans_df = pd.read_csv('../input/ted-talks/transcripts.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df = pd.merge(descr_df, trans_df, on=\"url\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stop_words(text):\n  \"\"\"\n    Remueve stop words en español\n\n    Attributes\n    ----------\n    text: list\n      lista de palabras (tokens) a filtrar\n\n    Returns\n    -------\n    list\n      lista de palabras sin los stop words\n  \"\"\"\n  return [token for token in text if token.lower() not in english_stops]","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def lematize_words(text):\n  \"\"\"\n    Lematización de palabras - aplica lematización de palabras sobre un set de tokens\n\n    Attributes\n    ----------\n    text: list\n      lista de palabras (tokens) sobre los cuales se aplicará la lematización\n    \n    Returns\n    -------\n    list\n      lista con todas las lematizaciones de las palabras\n  \"\"\"\n  doc = nlp(\" \".join(text))\n  return [token.lemma_ for token in doc]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stem_words(text):\n  \"\"\"\n    Stemming de palabras - aplica stemming de palabras sobre un set de tokens\n\n    Attributes\n    ----------\n    text: list\n      lista de palabras (tokens) sobre las que se aplicará stemming\n    \n    Returns\n    -------\n    list\n      lista con los stems de las palabras\n  \"\"\"\n  stemmer = SnowballStemmer('spanish')\n  return [stemmer.stem(word) for word in text]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_meaningless_words(text):\n  \"\"\"\n    Remueve palabras sin significado\n\n    Attributes\n    ----------\n    text: list\n      lista de palabras (tokens) a filtrar\n    \n    Returns\n    -------\n    list\n      lista de palabras filtrada en base a expresiones regulares\n  \"\"\"\n  patterns = [r\"(^={1,}=$)\", r'\\u200b']\n  tokens = text\n  for pattern in patterns:\n    regexp = re.compile(pattern)\n    tokens = [token for token in tokens if not regexp.search(token)]\n  return tokens","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(text, mode='word'): \n  \"\"\"\n    Tokenización de documento - tokeniza un documento por palabra o por oración\n\n    Attributes\n    ----------\n    text: str\n      documento a tokenizar\n    mode: str, optional\n      método de tokenización (default: 'word' (por palabra))\n    \n    Returns\n    -------\n    list\n      lista de tokens \n    \n    Raises\n    ------\n      Exception\n        si el mode no es 'word' o 'sentence'\n  \"\"\"\n  if mode == 'word':\n    return word_tokenize(text, language='english')\n  elif mode == 'sentence':\n    return sent_tokenize(text, language='english')\n  else:\n    raise Exception('metodo de tokenizacion no encontrado')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_procesamiento_texto(text):\n  \"\"\"\n    Pre-procesamiento y obtención de las 20 palabras más significativas\n\n    Attributes\n    ----------\n    text: str\n      documento a analizar\n\n    Returns\n    -------\n    pd.DataFrame\n      retorna un dataframe con las 20 palabras que más se repiten y su frecuencia\n  \"\"\"\n  tokenized = tokenize(text)\n  without_stops = remove_stop_words(tokenized)\n  meaningfull_tokens = remove_meaningless_words(without_stops)\n  lematized_words = lematize_words(meaningfull_tokens)\n  # stemmed_words = stem_words(lematized_words\n  return lematized_words\n\nresults = pre_procesamiento_texto(merged_df.transcript)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "preprocesamiento.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YamSNq8qPcjO"
      },
      "source": [
        "! pip install -U spacy\n",
        "! pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mpS-KN-QPcjQ"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "import nltk\n",
        "from nltk.text import Text\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.collocations import BigramCollocationFinder,BigramAssocMeasures\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('all')\n",
        "pp = pprint.PrettyPrinter(indent=4, compact=True)\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "english_stops = set(stopwords.words('english'))\n",
        "english_stops.update(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmZjR1hyu0pq"
      },
      "source": [
        "# Activacndo Google Drive para guardar la información\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qMbYIIHwPcjR"
      },
      "source": [
        "tedx_df = pd.read_csv('/content/gdrive/My Drive/Facultad/Austral/Maestria 1/13 - Text mining/00 - TPs/Web Scraping/TED_Talk.csv')\n",
        "tedx_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XODk3OYsPcjS"
      },
      "source": [
        "def remove_stop_words(text):\n",
        "  \"\"\"\n",
        "    Remueve stop words en inglés\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: list\n",
        "      lista de palabras (tokens) a filtrar\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista de palabras sin los stop words\n",
        "  \"\"\"\n",
        "  return [token for token in text if token.lower() not in english_stops]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RiXgrc6zPcjT"
      },
      "source": [
        "def lematize_words(text):\n",
        "  \"\"\"\n",
        "    Lematización de palabras - aplica lematización de palabras sobre un set de tokens\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: list\n",
        "      lista de palabras (tokens) sobre los cuales se aplicará la lematización\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista con todas las lematizaciones de las palabras\n",
        "  \"\"\"\n",
        "  doc = nlp(\" \".join(text))\n",
        "  return [token.lemma_ for token in doc]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DI9qGg9lPcjT"
      },
      "source": [
        "def stem_words(text):\n",
        "  \"\"\"\n",
        "    Stemming de palabras - aplica stemming de palabras sobre un set de tokens\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: list\n",
        "      lista de palabras (tokens) sobre las que se aplicará stemming\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista con los stems de las palabras\n",
        "  \"\"\"\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  return [stemmer.stem(word) for word in text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "lxBj3K1CPcjT"
      },
      "source": [
        "def remove_meaningless_words(text):\n",
        "  \"\"\"\n",
        "    Remueve palabras sin significado\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: list\n",
        "      lista de palabras (tokens) a filtrar\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista de palabras filtrada en base a expresiones regulares\n",
        "  \"\"\"\n",
        "  patterns = [r\"(^={1,}=$)\", r'\\u200b']\n",
        "  tokens = text\n",
        "  for pattern in patterns:\n",
        "    regexp = re.compile(pattern)\n",
        "    tokens = [token for token in tokens if not regexp.search(token)]\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oZzaWUJ-PcjU"
      },
      "source": [
        "def tokenize(text, mode='word'): \n",
        "  \"\"\"\n",
        "    Tokenización de documento - tokeniza un documento por palabra o por oración\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: str\n",
        "      documento a tokenizar\n",
        "    mode: str, optional\n",
        "      método de tokenización (default: 'word' (por palabra))\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "      lista de tokens \n",
        "    \n",
        "    Raises\n",
        "    ------\n",
        "      Exception\n",
        "        si el mode no es 'word' o 'sentence'\n",
        "  \"\"\"\n",
        "  if mode == 'word':\n",
        "    return word_tokenize(text, language='english')\n",
        "  elif mode == 'sentence':\n",
        "    return sent_tokenize(text, language='english')\n",
        "  else:\n",
        "    raise Exception('metodo de tokenizacion no encontrado')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ztZFXvaJwL"
      },
      "source": [
        "def similarity_btw_docs(matrix):\n",
        "  \"\"\"\n",
        "    Similitud entre documentos - calcula la similitud entre documentos utilizando Similitud del Coseno\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    matrix: scipy matrix\n",
        "      Matriz dispersa para calcular la similaridad\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "      retorna un dataframe con el grado de similaridad entre documentos (de 0 a 1)\n",
        "  \"\"\"\n",
        "  matrix_simil = cosine_similarity(matrix)\n",
        "  return pd.DataFrame(matrix_simil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Bf4WsWGvPcjU"
      },
      "source": [
        "def pre_procesamiento_texto(text):\n",
        "  \"\"\"\n",
        "    Pre-procesamiento y obtención de las 20 palabras más significativas\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    text: str\n",
        "      documento a analizar\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "      retorna un dataframe con las 20 palabras que más se repiten y su frecuencia\n",
        "  \"\"\"\n",
        "  tokenized = tokenize(text)\n",
        "  without_stops = remove_stop_words(tokenized)\n",
        "  meaningfull_tokens = remove_meaningless_words(without_stops)\n",
        "  lematized_words = lematize_words(meaningfull_tokens)\n",
        "  # stemmed_words = stem_words(lematized_words)\n",
        "  return lematized_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkD-qoC0cgR9"
      },
      "source": [
        "tedx_df = tedx_df.assign(talks__tags=tedx_df['talks__tags'].str.split(',')).explode('talks__tags')\n",
        "tedx_df['tag'] = tedx_df['talks__tags'].str.replace(\"[\", \"\").str.replace(\"'\", \"\").str.replace(\"]\", \"\")\n",
        "del tedx_df['talks__tags']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huR-2Kyjf_eu"
      },
      "source": [
        "tedx_df = tedx_df[tedx_df['transcript'] != np.nan]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0qZKCaymPcjV"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tedx_df.transcript, tedx_df.tag, test_size=.30)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHNQmZxNfszY"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=pre_procesamiento_texto)\n",
        "X_train_transform = tfidf_vectorizer.fit_transform(X_train.values.astype('U'))\n",
        "X_test_transform = tfidf_vectorizer.transform(X_test.values.astype('U'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X6IKl2apPcjV"
      },
      "source": [
        "similarity_btw_docs(X_train_transform)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}